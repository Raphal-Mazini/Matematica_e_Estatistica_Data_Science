{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0755812",
   "metadata": {},
   "source": [
    "# <font color='blue'>Matemática Para Data Science</font>\n",
    "\n",
    "## <font color='blue'>Estudo de Caso 8</font>\n",
    "### <font color='blue'>Implementando o Algoritmo Backpropagation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8efee249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa0716",
   "metadata": {},
   "source": [
    "### Algoritmo Backpropagation\n",
    "\n",
    "O algoritmo Backpropagation, ou retropropagação, é um método amplamente utilizado no treinamento de redes neurais. É um algoritmo de otimização baseado em gradientes que minimiza a função de custo, ajustando iterativamente os pesos da rede. O Backpropagation é utilizado em conjunto com um algoritmo de otimização, como o Gradiente Descendente, para realizar essa tarefa.\n",
    "\n",
    "Aqui está como o algoritmo Backpropagation funciona, dividido em etapas:\n",
    "\n",
    "**1- Feedforward:**\n",
    "\n",
    "- Começa com a entrada através da rede, camada por camada, até chegar à saída.\n",
    "- Utiliza os pesos atuais da rede para calcular a saída para cada neurônio.\n",
    "\n",
    "**2- Cálculo do Erro:**\n",
    "\n",
    "- Compara a saída calculada da rede com a saída desejada (rótulo verdadeiro).\n",
    "- Utiliza uma função de custo (como a entropia cruzada ou erro quadrático médio) para calcular o erro total da rede.\n",
    "\n",
    "**3- Retropropagação do Erro:**\n",
    "\n",
    "- Calcula o gradiente da função de custo em relação a cada peso, movendo-se da camada de saída para a camada de entrada.\n",
    "- Utiliza a regra da cadeia para calcular esses gradientes, o que envolve calcular derivadas parciais de várias quantidades, como a função de ativação e a função de custo.\n",
    "\n",
    "A propagação do gradiente através das camadas é o que dá o nome de \"retropropagação\".\n",
    "\n",
    "**4- Atualização dos Pesos:**\n",
    "\n",
    "- Utiliza os gradientes calculados junto com um algoritmo de otimização (como o Gradiente Descendente) para ajustar os pesos na direção que reduz o erro.\n",
    "- O tamanho da mudança em cada peso é determinado pela taxa de aprendizado, que é um hiperparâmetro escolhido manualmente.\n",
    "\n",
    "**5- Iteração:**\n",
    "\n",
    "- Repete as etapas de 1 a 4 para várias épocas ou até que o erro da rede atinja um valor aceitável.\n",
    "\n",
    "O algoritmo Backpropagation foi uma inovação crítica que permitiu o treinamento eficiente de redes neurais profundas e é uma parte central da maioria dos frameworks modernos de aprendizado profundo. Ao ajustar os pesos da rede de maneira que minimizem a função de custo, ele permite que a rede aprenda representações complexas dos dados de entrada e faça previsões precisas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82948a75",
   "metadata": {},
   "source": [
    "## Pseudo-Código\n",
    "\n",
    "![DSA](imagens/pseudo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163b93e7",
   "metadata": {},
   "source": [
    "O algoritmo Backpropagation é um método iterativo para treinar redes neurais. Abaixo está o pseudo-código para o algoritmo:\n",
    "\n",
    "Inicialize os pesos da rede:\n",
    "\n",
    "Inicialize os pesos da rede com pequenos valores aleatórios.\n",
    "Para cada época até a convergência, faça:\n",
    "\n",
    "## a. Para cada exemplo de treinamento na base de dados, faça:\n",
    "\n",
    "**i. Feedforward:**\n",
    "\n",
    "- Entrada: X (exemplo de entrada).\n",
    "- Para cada camada da rede, calcule a saída de cada neurônio com os pesos e a função de ativação.\n",
    "- Guarde as ativações e as saídas de cada camada.\n",
    "\n",
    "**ii. Calcule o erro da camada de saída:**\n",
    "\n",
    "- Calcule o erro da saída da rede comparando com o rótulo verdadeiro Y.\n",
    "- Utilize uma função de custo, como erro quadrático médio.\n",
    "\n",
    "**iii. Retropropagação do Erro:**\n",
    "\n",
    "- Para cada camada, começando pela camada de saída e movendo-se para trás:\n",
    "- Calcule o gradiente do erro em relação às ativações da camada.\n",
    "- Calcule o gradiente do erro em relação aos pesos da camada, usando a regra da cadeia.\n",
    "- Guarde os gradientes de cada camada.\n",
    "\n",
    "**iv. Atualização dos Pesos:**\n",
    "\n",
    "- Para cada camada:\n",
    "- Atualize os pesos da camada usando o gradiente calculado e a taxa de aprendizado.\n",
    "- Os pesos podem ser atualizados usando métodos como Gradiente Descendente.\n",
    "\n",
    "## b. Verifique a Convergência:\n",
    "\n",
    "Avalie a rede no conjunto de validação, se disponível.\n",
    "Se o erro no conjunto de validação estiver abaixo de um limiar, ou se o erro parar de diminuir, pare o treinamento.\n",
    "Retorne os pesos treinados da rede.\n",
    "\n",
    "O pseudo-código acima descreve o treinamento de uma rede neural usando Backpropagation. É um resumo de alto nível, e a implementação real pode exigir detalhes adicionais, como escolha da função de ativação, inicialização de peso, técnica de otimização, etc. A eficácia do treinamento pode ser sensível à escolha desses detalhes e hiperparâmetros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bedf5bb",
   "metadata": {},
   "source": [
    "## Implementação em Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41ddc94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de ativação\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d19fcd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivada da função sigmóide\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f6acf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de neurônios em cada camada da rede\n",
    "input_neurons, hidden_neurons, output_neurons = 2, 2, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0a15745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialização de pesos e bias da camada de entrada para a camada oculta\n",
    "weights_input_hidden = np.random.uniform(size = (input_neurons, hidden_neurons))\n",
    "bias_input_hidden = np.random.uniform(size = (1, hidden_neurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9c17b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialização de pesos e bias da camada oculta para a camada de saída\n",
    "weights_hidden_output = np.random.uniform(size = (hidden_neurons, output_neurons))\n",
    "bias_hidden_output = np.random.uniform(size = (1, output_neurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc325372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "015faf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output esperado\n",
    "y = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e94e23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparâmetros\n",
    "lr = 0.7\n",
    "n_epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6545b366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop de treino\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    # Feedforward\n",
    "    # Calcula a ativação da camada oculta (hidden layer) usando os pesos de entrada\n",
    "    hidden_layer_activation = np.dot(X, weights_input_hidden)\n",
    "    \n",
    "    # Adiciona o bias à ativação da camada oculta\n",
    "    hidden_layer_activation += bias_input_hidden\n",
    "    \n",
    "    # Aplica a função sigmoid para calcular a saída da camada oculta\n",
    "    hidden_layer_output = sigmoid(hidden_layer_activation)\n",
    "\n",
    "    # Calcula a ativação da camada de saída usando os pesos entre a camada oculta e a camada de saída\n",
    "    output_layer_activation = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "    \n",
    "    # Adiciona o bias à ativação da camada de saída\n",
    "    output_layer_activation += bias_hidden_output\n",
    "    \n",
    "    # Aplica a função sigmoid para calcular a saída prevista\n",
    "    predicted_output = sigmoid(output_layer_activation)\n",
    "\n",
    "    # Backpropagation\n",
    "    # Calcula o erro entre a saída prevista e a saída verdadeira\n",
    "    error = y - predicted_output\n",
    "    \n",
    "    # Calcula a derivada do erro em relação à saída prevista, usando a derivada da função sigmoid\n",
    "    d_predicted_output = error * sigmoid_derivative(predicted_output)\n",
    "\n",
    "    # Calcula o erro da camada oculta, propagando o erro da camada de saída\n",
    "    error_hidden_layer = d_predicted_output.dot(weights_hidden_output.T)\n",
    "    \n",
    "    # Calcula a derivada do erro em relação à saída da camada oculta\n",
    "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "    # Atualizando os pesos e os bias\n",
    "    # Atualiza os pesos da camada oculta para a camada de saída usando a taxa de aprendizado\n",
    "    weights_hidden_output += hidden_layer_output.T.dot(d_predicted_output) * lr\n",
    "    \n",
    "    # Atualiza o bias da camada de saída usando a taxa de aprendizado\n",
    "    bias_hidden_output += np.sum(d_predicted_output, axis = 0, keepdims = True) * lr\n",
    "    \n",
    "    # Atualiza os pesos da camada de entrada para a camada oculta usando a taxa de aprendizado\n",
    "    weights_input_hidden += X.T.dot(d_hidden_layer) * lr\n",
    "    \n",
    "    # Atualiza o bias da camada oculta usando a taxa de aprendizado\n",
    "    bias_input_hidden += np.sum(d_hidden_layer, axis = 0, keepdims = True) * lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca407e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0158039 ]\n",
      " [0.98643808]\n",
      " [0.98643507]\n",
      " [0.01399347]]\n"
     ]
    }
   ],
   "source": [
    "print(predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18a4f9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0047130d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.70812482 6.59053076]\n",
      " [4.70895404 6.59391294]]\n"
     ]
    }
   ],
   "source": [
    "print(weights_input_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29cd7bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-10.66308156]\n",
      " [  9.95714187]]\n"
     ]
    }
   ],
   "source": [
    "print(weights_hidden_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "394550ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.22511647 -2.94765901]]\n"
     ]
    }
   ],
   "source": [
    "print(bias_input_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ccd6cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.62020316]]\n"
     ]
    }
   ],
   "source": [
    "print(bias_hidden_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab39478e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
